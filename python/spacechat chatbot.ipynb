{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0,0], [batch_size, -1], [1,1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length, attn_length):\n",
    "    lstm = tf.contrib.rnn.BasicLstmCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnMultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n",
    "                                                  cell_bw = enc_cell,\n",
    "                                                  sequence_length = sequence_length,\n",
    "                                                  inputs = rnn_inputs,\n",
    "                                                  dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, \n",
    "                                1, \n",
    "                                dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "        tf.contrib.seq2seq.prepare_attention(\n",
    "             attention_states,\n",
    "             attention_option=\"bahdanau\",\n",
    "             num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = \\  \n",
    "       tf.contrib.seq2seq.attention_decoder_fn_train(\n",
    "             encoder_state[0],\n",
    "             att_keys,\n",
    "             att_vals,\n",
    "             att_score_fn,\n",
    "             att_construct_fn,\n",
    "             name = \"attn_dec_train\")\n",
    "    \n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "             dec_cell, \n",
    "             train_decoder_fn, \n",
    "             dec_embed_input, \n",
    "             sequence_length, \n",
    "             scope=decoding_scope)\n",
    "    \n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    \n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings,  \n",
    "                         start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope,        \n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, \n",
    "                                1, \n",
    "                                dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "        tf.contrib.seq2seq.prepare_attention(\n",
    "            attention_states,\n",
    "            attention_option=\"bahdanau\",\n",
    "            num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = \\  \n",
    "        tf.contrib.seq2seq.attention_decoder_fn_inference(\n",
    "            output_fn, \n",
    "            encoder_state[0], \n",
    "            att_keys, \n",
    "            att_vals, \n",
    "            att_score_fn, \n",
    "            att_construct_fn, \n",
    "            dec_embeddings,\n",
    "            start_of_sequence_id, \n",
    "            end_of_sequence_id, \n",
    "            maximum_length, \n",
    "            vocab_size, \n",
    "            name = \"attn_dec_inf\")\n",
    "    \n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        dec_cell, \n",
    "        infer_decoder_fn, \n",
    "        scope=decoding_scope)\n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, \n",
    "                   vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    \n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "                   lstm, \n",
    "                   input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(\n",
    "                       x, \n",
    "                       vocab_size, \n",
    "                       None, \n",
    "                       scope=decoding_scope,\n",
    "                       weights_initializer = weights,\n",
    "                       biases_initializer = biases)\n",
    "        train_logits = decoding_layer_train(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "    \n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            vocab_to_int['<GO>'],\n",
    "                                            vocab_to_int['<EOS>'], \n",
    "                                            sequence_length - 1, \n",
    "                                            vocab_size,\n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, \n",
    "                  sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, \n",
    "                  dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(\n",
    "        input_data, \n",
    "        answers_vocab_size+1, \n",
    "        enc_embedding_size,\n",
    "        initializer = tf.random_uniform_initializer(-1,1))\n",
    "    \n",
    "    enc_state = encoding_layer(enc_embed_input, \n",
    "                               rnn_size,\n",
    "                               num_layers, \n",
    "                               keep_prob, \n",
    "                               sequence_length)\n",
    "    dec_input = process_encoding_input(target_data,      \n",
    "                                       questions_vocab_to_int, \n",
    "                                       batch_size)\n",
    "    dec_embeddings = tf.Variable(  \n",
    "        tf.random_uniform([questions_vocab_size+1,  \n",
    "                           dec_embedding_size], \n",
    "                          -1, 1))\n",
    "    \n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, \n",
    "                                             dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(\n",
    "        dec_embed_input, \n",
    "        dec_embeddings, \n",
    "        enc_state, \n",
    "        questions_vocab_size, \n",
    "        sequence_length, \n",
    "        rnn_size, \n",
    "        num_layers, \n",
    "        questions_vocab_to_int, \n",
    "        keep_prob, \n",
    "        batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "      \n",
    "input_data, input_length, targets, lr, keep_prob = model_inputs()\n",
    "sequence_length = tf.placeholder_with_default(\n",
    "        max_line_length, \n",
    "        None, \n",
    "        name='sequence_length')\n",
    "input_shape = tf.shape(input_data)\n",
    "train_logits, inference_logits = seq2seq_model(\n",
    "    tf.reverse(input_data, [-1]), \n",
    "    targets, \n",
    "    keep_prob, \n",
    "    batch_size, \n",
    "    sequence_length, \n",
    "    len(answers_vocab_to_int), \n",
    "    len(questions_vocab_to_int), \n",
    "    encoding_embedding_size, \n",
    "    decoding_embedding_size, \n",
    "    rnn_size, \n",
    "    num_layers, \n",
    "    questions_vocab_to_int)\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for \n",
    "        grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = np.random.choice(len(short_questions))\n",
    "input_question = short_questions[random]\n",
    "input_question = question_to_seq(input_question, \n",
    "                                 questions_vocab_to_int)\n",
    "input_question = input_question + \n",
    "                 [questions_vocab_to_int[\"<PAD>\"]] * \n",
    "                 (max_line_length - len(input_question))\n",
    "\n",
    "batch_shell = np.zeros((batch_size, max_line_length))\n",
    "batch_shell[0] = input_question    \n",
    "    \n",
    "answer_logits = sess.run(inference_logits, {input_data: batch_shell, \n",
    "                                            keep_prob: 1.0})[0]\n",
    "pad_q = questions_vocab_to_int[\"<PAD>\"]\n",
    "pad_a = answers_vocab_to_int[\"<PAD>\"]\n",
    "print('Question')\n",
    "print('  Word Ids: {}'.format(\n",
    " [i for i in input_question if i != pad_q]))\n",
    "print('  Input Words: {}'.format(\n",
    " [questions_int_to_vocab[i] for i in input_question if i != pad_q]))\n",
    "print('\\nAnswer')\n",
    "print('Word Ids: {}'.format(\n",
    "    [i for i in np.argmax(answer_logits, 1) if i != pad_a]))\n",
    "print('Response Words: {}'.format(\n",
    "    [answers_int_to_vocab[i] for i in \\\n",
    "     np.argmax(answer_logits, 1) if i != pad_a]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
